{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62162300-1725-446e-a4ee-930bac638a2b",
   "metadata": {},
   "source": [
    "### Based on Jude's July 29v2, edited by Yang Aug 5:\n",
    "- removed one-hot encoding of M columns, used 0/1 or numerical encoding, replaced nan in these columns with -1\n",
    "- splitted training data into train and test and then further split training into training and cross-validation set. The cross-validation set is used to choose the best epoch/best weight and the test set is used for evaluation the final model only, never used in training\n",
    "- replaced Autoencoder class becasue I wasn't sure why the forward function was different from encoder+decoder\n",
    "- added reLU and dorpout after each linear layer in the encoder as well as decoder\n",
    "- saved best weights during training to be loaded later for quick model evaluation\n",
    "- minor rearrangement and varaible renaming for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83df4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd33dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train_transaction.csv\")#, nrows=10000)\n",
    "\n",
    "# derive “day” from TransactionDT, then drop the raw column\n",
    "df[\"day\"] = (df[\"TransactionDT\"] // (3600 * 24)).astype(int)\n",
    "df.drop(\"TransactionDT\", axis=1, inplace=True)\n",
    "\n",
    "# drop TransactionID, as it is not useful for modeling\n",
    "df.drop(\"TransactionID\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b02aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a435cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b81da5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute missing % for all columns\n",
    "nulls = df.isna().mean() * 100\n",
    "\n",
    "# find columns with more than 80% missing values\n",
    "cols_80 = nulls[nulls >= 80].index.tolist()\n",
    "\n",
    "# and drop them!\n",
    "df.drop(columns=cols_80, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b01e3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric imputation (median) – exclude the target “isFraud”\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols.remove(\"isFraud\")\n",
    "\n",
    "# among num_cols, find columns with nans that need to be imputed\n",
    "nan_cols = [c for c in num_cols if df[c].isna().any()]\n",
    "\n",
    "# exclude the categorical columns card2, card3, card5, addr1, addr2\n",
    "cat_cols = [\"card2\", \"card3\", \"card5\", \"addr1\", \"addr2\"]\n",
    "nan_cols = [c for c in nan_cols if c not in cat_cols]\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "df[nan_cols] = imputer.fit_transform(df[nan_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26124b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for remaining categoricals, one‐hot encode small‐cardinaliy ones else drop them\n",
    "cat_cols_rem = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# include the cat_cols that were excluded earlier\n",
    "cat_cols_rem.extend(cat_cols)\n",
    "\n",
    "# e.g. “ProductCD”, “MISSING” placeholders, etc.\n",
    "for c in cat_cols_rem:\n",
    "    n_uniq = df[c].nunique()\n",
    "    if n_uniq <= 10:\n",
    "        # ------------------ yy -----------------------\n",
    "        if c[0] == 'M':\n",
    "            if c[1] != 4:\n",
    "                df[c] = df[c].map({'T': 1, 'F': 0})\n",
    "            else:\n",
    "                df[c] = df[c].map({'M0': 0, 'M1': 1, 'M2':2})\n",
    "            df[c]=df[c].fillna(-1)\n",
    "        else:\n",
    "        # ---------------------------------------------\n",
    "            dummies = pd.get_dummies(df[c], prefix=c, drop_first=True)\n",
    "            df = pd.concat([df.drop(c, axis=1), dummies], axis=1)\n",
    "    else:\n",
    "        df.drop(columns=c, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e36a5eb-3a22-4a45-b5d3-dd3cd53221ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all bool columns in df\n",
    "bool_cols = df.select_dtypes(include=\"bool\").columns\n",
    "\n",
    "# cast them to int (True→1, False→0)\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0d4311d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set(64% normal data): (364720, 337)\n",
      "cv set(16% normal data): (91181, 337)\n",
      "test set(20% normal data + fraud): (134639, 337)\n"
     ]
    }
   ],
   "source": [
    "# Create training and CV sets\n",
    "# all non‐fraud examples\n",
    "df_norm = df[df.isFraud == 0].copy()\n",
    "# all fraud examples\n",
    "df_fraud = df[df.isFraud == 1].copy()\n",
    "\n",
    "# --------------------- yy -----------------------\n",
    "# hold out 20% of normals for test\n",
    "# hold out 20% of training for cross validation, leaving test for assessing model performance only, not model selection or tuning.\n",
    "norm_train, norm_test = train_test_split(df_norm, test_size=0.2, random_state=42)\n",
    "norm_train, norm_cv = train_test_split(norm_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# test set = held‐out test + all frauds\n",
    "df_test = pd.concat([norm_test, df_fraud], axis=0)\n",
    "X_test = df_test.drop(\"isFraud\", axis=1)\n",
    "y_test = df_test[\"isFraud\"].values\n",
    "# SHOULD THE LABEL IN df_test BE DROPPED AS WELL?\n",
    "\n",
    "# drop labels for modeling\n",
    "X_train = norm_train.drop(\"isFraud\", axis=1)\n",
    "X_cv    = norm_cv.drop(\"isFraud\", axis=1)\n",
    "\n",
    "print(f'train set(64% normal data): {X_train.shape}')\n",
    "print(f'cv set(16% normal data): {X_cv.shape}')\n",
    "print(f'test set(20% normal data + fraud): {X_test.shape}')\n",
    "# ------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b69b8962-e825-4f78-b95a-4f8023c6dd5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_hot_cols = [col for col in X_train.columns if set(X_train[col].unique()) <= {0, 1}]\n",
    "non_one_hot_cols = [col for col in X_train.columns if col not in one_hot_cols]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_cv_scaled    = X_cv.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[non_one_hot_cols] = scaler.fit_transform(X_train[non_one_hot_cols])\n",
    "X_cv_scaled[non_one_hot_cols]    = scaler.transform(X_cv[non_one_hot_cols])\n",
    "X_test_scaled[non_one_hot_cols]    = scaler.transform(X_test[non_one_hot_cols])\n",
    "\n",
    "X_train_final = X_train_scaled.values\n",
    "X_cv_final    = X_cv_scaled.values\n",
    "X_test_final = X_test_scaled.values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "# --------------------------- yy ---------------------------\n",
    "x_train = torch.tensor(X_train_final, dtype=torch.float32)\n",
    "x_valid = torch.tensor(X_cv_final, dtype=torch.float32)\n",
    "x_test = torch.tensor(X_test_final, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb9baaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output must match the input for autoencoders\n",
    "\n",
    "class FraudDatasetUnsupervised(Dataset):\n",
    "    \n",
    "    def __init__(self, x,output=True):\n",
    "        'Initialization'\n",
    "        self.x = x\n",
    "        self.output = output\n",
    "\n",
    "    def __len__(self):\n",
    "        'Returns the total number of samples'\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample index\n",
    "        item = self.x[index]\n",
    "        if self.output:\n",
    "            return item, item\n",
    "        else:\n",
    "            return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03b3d91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = FraudDatasetUnsupervised(x_train)\n",
    "valid_set = FraudDatasetUnsupervised(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82ad6e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Pytorch loaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(valid_set,   batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae7f9ac-a029-43ea-8889-778e7d77169a",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a8f9c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Jude's autoencoder\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass DropAutoencoder(nn.Module):\\n    def __init__(self, input_size, intermediate_size_1, intermediate_size_2, code_size, dropout_rate=0.2):\\n        super(DropAutoencoder, self).__init__()\\n\\n        # Encoder\\n        self.fc1 = nn.Linear(input_size, intermediate_size_1)\\n        self.fc2 = nn.Linear(intermediate_size_1, intermediate_size_2)\\n        self.fc3 = nn.Linear(intermediate_size_2, code_size)\\n        \\n        # Decoder\\n        self.fc4 = nn.Linear(code_size, intermediate_size_2)\\n        self.fc5 = nn.Linear(intermediate_size_2, intermediate_size_1)\\n        self.fc6 = nn.Linear(intermediate_size_1, input_size)\\n        \\n        self.dropout = nn.Dropout(dropout_rate)\\n\\n    def forward(self, x):\\n        # Encoder with dropout noise\\n        x = F.relu(self.fc1(x))\\n        x = self.dropout(x)\\n        \\n        x = F.relu(self.fc2(x))\\n        x = self.dropout(x)\\n        \\n        code = F.relu(self.fc3(x))\\n        \\n        # Decoder\\n        x = F.relu(self.fc4(code))\\n        x = self.dropout(x)\\n        \\n        x = F.relu(self.fc5(x))\\n        x = self.dropout(x)\\n        \\n        output = self.fc6(x)  # Linear activation\\n        return output\\n        \""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' # Jude's autoencoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DropAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size_1, intermediate_size_2, code_size, dropout_rate=0.2):\n",
    "        super(DropAutoencoder, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_size, intermediate_size_1)\n",
    "        self.fc2 = nn.Linear(intermediate_size_1, intermediate_size_2)\n",
    "        self.fc3 = nn.Linear(intermediate_size_2, code_size)\n",
    "        \n",
    "        # Decoder\n",
    "        self.fc4 = nn.Linear(code_size, intermediate_size_2)\n",
    "        self.fc5 = nn.Linear(intermediate_size_2, intermediate_size_1)\n",
    "        self.fc6 = nn.Linear(intermediate_size_1, input_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder with dropout noise\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        code = F.relu(self.fc3(x))\n",
    "        \n",
    "        # Decoder\n",
    "        x = F.relu(self.fc4(code))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        output = self.fc6(x)  # Linear activation\n",
    "        return output\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c978d35-49c8-42f9-bce1-fc73f27894df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------- YY ------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#import torch.nn.functional as F\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size_1, intermediate_size_2, latent_size, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, intermediate_size_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = dropout_rate),\n",
    "            nn.Linear(intermediate_size_1, intermediate_size_2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = dropout_rate),\n",
    "            nn.Linear(intermediate_size_2, latent_size),\n",
    "        )        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_size, intermediate_size_2),            \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = dropout_rate),\n",
    "            nn.Linear(intermediate_size_2, intermediate_size_1),       \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p = dropout_rate),\n",
    "            nn.Linear(intermediate_size_1, input_size),\n",
    "        )\n",
    "        #self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        return self.decoder(latent)\n",
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9665bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_sample_mse(model, generator):\n",
    "    \n",
    "    model.eval()\n",
    "    criterion = torch.nn.MSELoss(reduction=\"none\")\n",
    "    batch_losses = []\n",
    "    \n",
    "    for x_batch, y_batch in generator:\n",
    "        # Forward pass\n",
    "        y_pred = model(x_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(y_pred.squeeze(), y_batch)\n",
    "        loss_app = list(torch.mean(loss,axis=1).detach().cpu().numpy())\n",
    "        batch_losses.extend(loss_app)\n",
    "    \n",
    "    return batch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66da7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,generator,criterion):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    for x_batch, y_batch in generator:\n",
    "        # Forward pass\n",
    "        y_pred = model(x_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(y_pred.squeeze(), y_batch)\n",
    "        batch_losses.append(loss.item())\n",
    "    mean_loss = np.mean(batch_losses)    \n",
    "    return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04b30bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \n",
    "    def __init__(self, patience=3, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = np.inf\n",
    "    # ------------------------- YY ---------------------\n",
    "    def continue_training(self,current_score, save=False):\n",
    "        if self.best_score > current_score:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(\"New best score:\", current_score)\n",
    "            save_flag = True # signal outside loop to save training weight\n",
    "        else:\n",
    "            self.counter+=1\n",
    "            if self.verbose:\n",
    "                print(self.counter, \" iterations since best score.\")\n",
    "            save_flag = False\n",
    "        #-----------------------------------------------\n",
    "        return self.counter <= self.patience, save_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4465904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model,training_generator,valid_generator,optimizer,criterion,max_epochs=100,\n",
    "                  apply_early_stopping=True,patience=3,verbose=False):\n",
    "    #Setting the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    if apply_early_stopping:\n",
    "        early_stopping = EarlyStopping(verbose=verbose,patience=patience)\n",
    "    \n",
    "    all_train_losses = []\n",
    "    all_valid_losses = []\n",
    "    \n",
    "    #Training loop\n",
    "    start_time=time.time()\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_loss=[]\n",
    "        for x_batch, y_batch in training_generator:\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            y_pred = model(x_batch)\n",
    "            # Compute Loss\n",
    "            loss = criterion(y_pred.squeeze(), y_batch)\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()   \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        #showing last training loss after each epoch\n",
    "        all_train_losses.append(np.mean(train_loss))\n",
    "        if verbose:\n",
    "            print('')\n",
    "            print('Epoch {}: train loss: {}'.format(epoch, np.mean(train_loss)))\n",
    "        #evaluating the model on the test set after each epoch    \n",
    "        valid_loss = evaluate_model(model,valid_generator,criterion)\n",
    "        all_valid_losses.append(valid_loss)\n",
    "        if verbose:\n",
    "            print('valid loss: {}'.format(valid_loss))\n",
    "        # --------------yy ----------------------\n",
    "        if apply_early_stopping:\n",
    "            early_stop, save_weights = early_stopping.continue_training(valid_loss)\n",
    "            if save_weights:\n",
    "                torch.save(model.state_dict(), f'autoencoder_best_weights.pth')\n",
    "            if not early_stop:\n",
    "                if verbose:\n",
    "                    print(\"Early stopping\")\n",
    "                break\n",
    "        \n",
    "    training_execution_time=time.time()-start_time\n",
    "    return model,training_execution_time,all_train_losses,all_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a15fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() # this is aggregated loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77e91dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''seed_everything(SEED)'''\n",
    "model = Autoencoder(x_train.shape[1], 128, 64, 16, dropout_rate=0.2)\n",
    "losses = per_sample_mse(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "721b65cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49203458, 0.594671, 0.13820206, 0.13903299, 1.0079634]\n",
      "1.1732432\n"
     ]
    }
   ],
   "source": [
    "print(losses[0:5])\n",
    "print(np.mean(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d616349",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef8007f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: train loss: 0.6384514754180177\n",
      "valid loss: 0.6636157513971914\n",
      "New best score: 0.6636157513971914\n",
      "\n",
      "Epoch 1: train loss: 0.4970251948064954\n",
      "valid loss: 0.58618536463955\n",
      "New best score: 0.58618536463955\n",
      "\n",
      "Epoch 2: train loss: 0.44899287717889164\n",
      "valid loss: 0.5269513091967817\n",
      "New best score: 0.5269513091967817\n",
      "\n",
      "Epoch 3: train loss: 0.4251902113412217\n",
      "valid loss: 0.4866589359180969\n",
      "New best score: 0.4866589359180969\n",
      "\n",
      "Epoch 4: train loss: 0.4096893847621435\n",
      "valid loss: 0.46571934109717084\n",
      "New best score: 0.46571934109717084\n",
      "\n",
      "Epoch 5: train loss: 0.3991813416213482\n",
      "valid loss: 0.444648709919369\n",
      "New best score: 0.444648709919369\n",
      "\n",
      "Epoch 6: train loss: 0.38653401792922715\n",
      "valid loss: 0.4182989833177182\n",
      "New best score: 0.4182989833177182\n",
      "\n",
      "Epoch 7: train loss: 0.37108506382839956\n",
      "valid loss: 0.3931804719985577\n",
      "New best score: 0.3931804719985577\n",
      "\n",
      "Epoch 8: train loss: 0.3603577736512174\n",
      "valid loss: 0.3762360926573737\n",
      "New best score: 0.3762360926573737\n",
      "\n",
      "Epoch 9: train loss: 0.3539414948869283\n",
      "valid loss: 0.3643875238717648\n",
      "New best score: 0.3643875238717648\n",
      "\n",
      "Epoch 10: train loss: 0.34570371719892035\n",
      "valid loss: 0.3574773685503424\n",
      "New best score: 0.3574773685503424\n",
      "\n",
      "Epoch 11: train loss: 0.34160069207190047\n",
      "valid loss: 0.34453213216442813\n",
      "New best score: 0.34453213216442813\n",
      "\n",
      "Epoch 12: train loss: 0.3365250600063606\n",
      "valid loss: 0.33673325828292916\n",
      "New best score: 0.33673325828292916\n",
      "\n",
      "Epoch 13: train loss: 0.3312682673010853\n",
      "valid loss: 0.32795439562253786\n",
      "New best score: 0.32795439562253786\n",
      "\n",
      "Epoch 14: train loss: 0.32896644865123614\n",
      "valid loss: 0.3236853271199946\n",
      "New best score: 0.3236853271199946\n",
      "\n",
      "Epoch 15: train loss: 0.32668098027893566\n",
      "valid loss: 0.31197231080971266\n",
      "New best score: 0.31197231080971266\n",
      "\n",
      "Epoch 16: train loss: 0.32605943133965315\n",
      "valid loss: 0.30977787826144904\n",
      "New best score: 0.30977787826144904\n",
      "\n",
      "Epoch 17: train loss: 0.32510756689203396\n",
      "valid loss: 0.30851264265545625\n",
      "New best score: 0.30851264265545625\n",
      "\n",
      "Epoch 18: train loss: 0.32186673798620086\n",
      "valid loss: 0.2930815179097025\n",
      "New best score: 0.2930815179097025\n",
      "\n",
      "Epoch 19: train loss: 0.32214008448186265\n",
      "valid loss: 0.3089096518775873\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 20: train loss: 0.31796234323960015\n",
      "valid loss: 0.295122449235958\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 21: train loss: 0.31817613567834696\n",
      "valid loss: 0.2978524718472832\n",
      "3  iterations since best score.\n",
      "\n",
      "Epoch 22: train loss: 0.3187359126343876\n",
      "valid loss: 0.2873852517102894\n",
      "New best score: 0.2873852517102894\n",
      "\n",
      "Epoch 23: train loss: 0.31286896792032737\n",
      "valid loss: 0.2824476372149953\n",
      "New best score: 0.2824476372149953\n",
      "\n",
      "Epoch 24: train loss: 0.3104565367507901\n",
      "valid loss: 0.27721050080500154\n",
      "New best score: 0.27721050080500154\n",
      "\n",
      "Epoch 25: train loss: 0.30971228313024557\n",
      "valid loss: 0.27291863562767965\n",
      "New best score: 0.27291863562767965\n",
      "\n",
      "Epoch 26: train loss: 0.3073710711389534\n",
      "valid loss: 0.27395210758635874\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 27: train loss: 0.3063810259315565\n",
      "valid loss: 0.26672565014738786\n",
      "New best score: 0.26672565014738786\n",
      "\n",
      "Epoch 28: train loss: 0.30194465102339274\n",
      "valid loss: 0.2736123916140774\n",
      "1  iterations since best score.\n",
      "\n",
      "Epoch 29: train loss: 0.3047451270496789\n",
      "valid loss: 0.2710574686370398\n",
      "2  iterations since best score.\n",
      "\n",
      "Epoch 30: train loss: 0.30127533025559805\n",
      "valid loss: 0.26806052834318395\n",
      "3  iterations since best score.\n",
      "\n",
      "Epoch 31: train loss: 0.30417293892981484\n",
      "valid loss: 0.2690943087402143\n",
      "4  iterations since best score.\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "model,training_execution_time,train_losses,valid_losses = training_loop(model,train_loader,val_loader,optimizer,criterion,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4baa1f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.035916753, 0.1652522, 0.018691897, 0.018018825, 0.019540202]\n",
      "0.26912415\n",
      "[2612.7449, 1082.4082, 1082.3351, 507.0893, 324.06323, 161.85315, 111.87723, 96.305084, 82.36629, 62.981995]\n"
     ]
    }
   ],
   "source": [
    "losses = per_sample_mse(model, val_loader)\n",
    "print(losses[0:5])\n",
    "print(np.mean(losses))\n",
    "losses_sorted = sorted(losses, reverse=True)\n",
    "print(losses_sorted[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3490c34c-96df-4c57-ba64-24d6f269fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------- YY --------------------------\n",
    "loaded_model = Autoencoder(x_train.shape[1], 128, 64, 16, dropout_rate=0.2)\n",
    "loaded_model.load_state_dict(torch.load(f'autoencoder_best_weights.pth'))\n",
    "loaded_model.eval()\n",
    "\n",
    "output_test = loaded_model(x_test)\n",
    "crit = torch.nn.MSELoss(reduction=\"none\")\n",
    "loss = crit(output_test, x_test)\n",
    "sample_loss = loss.mean(axis=1).detach().numpy()\n",
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9855e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average fraud reconstruction error: 3.0505252\n",
      "Average genuine reconstruction error: 0.23194109\n"
     ]
    }
   ],
   "source": [
    "genuine_losses = np.array(sample_loss[y_test == 0])\n",
    "fraud_losses = np.array(sample_loss[y_test == 1])\n",
    "print(\"Average fraud reconstruction error:\", np.mean(fraud_losses))\n",
    "print(\"Average genuine reconstruction error:\", np.mean(genuine_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f88db505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import (average_precision_score, roc_auc_score)\n",
    "\n",
    "# compute AUC-ROC and Average Precision on the validation set by considering the reconstruction errors as predicted fraud scores\n",
    "\n",
    "AUC_ROC = roc_auc_score(y_test, sample_loss)\n",
    "AP = average_precision_score(y_test, sample_loss)\n",
    "    \n",
    "performances = pd.DataFrame([[AUC_ROC, AP]], columns=['AUC ROC','Average precision'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bd5bf37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUC ROC</th>\n",
       "      <th>Average precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.741293</td>\n",
       "      <td>0.475397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AUC ROC  Average precision\n",
       "0  0.741293           0.475397"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3ccea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at threshold 0.1807 = 0.6264\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "thr      = np.percentile(sample_loss, 70)      # e.g. top 30% as “fraud”\n",
    "y_pred   = (sample_loss >= thr).astype(int)\n",
    "recall   = recall_score(y_test, y_pred)    # binary‐class recall\n",
    "print(f\"Recall at threshold {thr:.4f} = {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072247b6-7908-4205-a99a-7911d737e1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
